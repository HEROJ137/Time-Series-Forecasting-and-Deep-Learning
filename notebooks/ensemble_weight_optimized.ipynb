{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2e3948",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost pytorch-lightning pytorch-forecasting optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Drive Mount & Path Setup\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/LGAI'\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "SUB_DIR = os.path.join(BASE_DIR, 'submissions')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(SUB_DIR, exist_ok=True)\n",
    "\n",
    "train_path  = os.path.join(BASE_DIR, 'train', 'train.csv')\n",
    "sample_path = os.path.join(BASE_DIR, 'sample_submission.csv')\n",
    "test_paths  = [os.path.join(BASE_DIR, 'test', f'TEST_0{i}.csv') for i in range(10)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Model Selection (Load or Train) ----",
    "from datetime import datetime",
    "",
    "def list_models(ext):",
    "    files = sorted([f for f in os.listdir(MODEL_DIR) if f.endswith(ext)])",
    "    return files",
    "",
    "cat_files = list_models('.cbm')",
    "xgb_files = list_models('.json')",
    "tft_files = list_models('.ckpt')",
    "",
    "if cat_files:",
    "    print('Available CatBoost models:', cat_files)",
    "else:",
    "    print('No saved CatBoost model found.')",
    "cat_choice = input('Enter CatBoost filename to load or press Enter to train: ').strip()",
    "if cat_choice == '' or cat_choice == 'train' or cat_choice not in cat_files:",
    "    cat_choice = None",
    "",
    "if xgb_files:",
    "    print('Available XGBoost models:', xgb_files)",
    "else:",
    "    print('No saved XGBoost model found.')",
    "xgb_choice = input('Enter XGBoost filename to load or press Enter to train: ').strip()",
    "if xgb_choice == '' or xgb_choice == 'train' or xgb_choice not in xgb_files:",
    "    xgb_choice = None",
    "",
    "if tft_files:",
    "    print('Available TFT-mini models:', tft_files)",
    "else:",
    "    print('No saved TFT-mini model found.')",
    "tft_choice = input('Enter TFT-mini filename to load or press Enter to train: ').strip()",
    "if tft_choice == '' or tft_choice == 'train' or tft_choice not in tft_files:",
    "    tft_choice = None",
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, gc, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "import optuna\n",
    "import glob, re\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc6a89",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed & Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 설정\n",
    "ENC_LEN  = 28\n",
    "PRED_LEN = 7\n",
    "ROLL_WINS = [7, 14, 28]\n",
    "\n",
    "# CatBoost 하이퍼파라미터\n",
    "CAT_PARAMS = dict(\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    iterations=2000,\n",
    "    loss_function=\"RMSE\",\n",
    "    l2_leaf_reg=5,\n",
    "    random_seed=42,\n",
    "    verbose=200,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    ")\n",
    "\n",
    "# TFT-mini 하이퍼파라미터 (dropout tuned later)\n",
    "TFT_PARAMS = dict(\n",
    "    learning_rate=2e-3,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=2,\n",
    "    hidden_continuous_size=32,\n",
    "    lstm_layers=1,\n",
    ")\n",
    "\n",
    "# 앙상블 가중치\n",
    "W_CAT = 0.4\n",
    "W_TFT = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3aef3",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753047b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv(train_path, parse_dates=['영업일자'])\n",
    "sample = pd.read_csv(sample_path)\n",
    "tests  = {f'TEST_0{i}': pd.read_csv(p, parse_dates=['영업일자']) for i, p in enumerate(test_paths)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3afdf",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff71d0f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# (1) 미래에 '알고 있는' 달력/이벤트 피처 목록\n",
    "# --------------------------------------------------------------\n",
    "known_future_cols = [\n",
    "    \"dow\",\"month\",\"is_weekend\",\"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# (2) CatBoost 학습용: shift 벡터화 + fragmentation 방지\n",
    "# --------------------------------------------------------------\n",
    "train_sorted = train.sort_values([\"영업장명_메뉴명\",\"영업일자\"]).reset_index(drop=True)\n",
    "g = train_sorted.groupby(\"영업장명_메뉴명\", sort=False)\n",
    "train_sorted[\"hist_ok\"] = g.cumcount() >= (ENC_LEN - 1)\n",
    "\n",
    "base_for_shift = [\"매출수량\"] + known_future_cols\n",
    "shift_blocks = []\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    block = g[base_for_shift].shift(-h)\n",
    "    rename_map = {\"매출수량\": f\"y_H{h}\"}\n",
    "    rename_map.update({c: f\"{c}_H{h}\" for c in known_future_cols})\n",
    "    block = block.rename(columns=rename_map)\n",
    "    shift_blocks.append(block)\n",
    "\n",
    "train_shift = pd.concat(shift_blocks, axis=1)\n",
    "train_sorted = pd.concat([train_sorted, train_shift], axis=1).copy()\n",
    "\n",
    "def build_catboost_Xy_by_shift(df: pd.DataFrame, stride: int = 1):\n",
    "    Xy = {}\n",
    "    base_cols = cat_features_cols + [f\"roll_mean_{w}\" for w in ROLL_WINS]\n",
    "    for c in base_cols:\n",
    "        if c in cat_features_cols:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    for h in range(1, PRED_LEN + 1):\n",
    "        fut_cols_h = [f\"{c}_H{h}\" for c in known_future_cols]\n",
    "        target_col = f\"y_H{h}\"\n",
    "        mask = df[\"hist_ok\"] & df[target_col].notna()\n",
    "        use = df.loc[mask, base_cols + fut_cols_h + [target_col]].copy() if stride==1 else (\n",
    "            df.loc[\n",
    "                df[mask].groupby(\"영업장명_메뉴명\", sort=False).apply(lambda x: x.iloc[::stride]).reset_index(level=0, drop=True).index,\n",
    "                base_cols + fut_cols_h + [target_col]\n",
    "            ].copy()\n",
    "        )\n",
    "        for c in fut_cols_h:\n",
    "            use[c] = pd.to_numeric(use[c], errors=\"coerce\").astype(\"float32\")\n",
    "        use[target_col] = pd.to_numeric(use[target_col], errors=\"coerce\").astype(\"float32\")\n",
    "        X = use[base_cols + fut_cols_h].reset_index(drop=True)\n",
    "        y = use[target_col].reset_index(drop=True)\n",
    "        Xy[h] = (X, y)\n",
    "    return Xy\n",
    "\n",
    "Xy_h = build_catboost_Xy_by_shift(train_sorted, stride=1)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# (3) TFT용 데이터셋/로더\n",
    "# --------------------------------------------------------------\n",
    "train_tft = train.copy()\n",
    "train_tft[\"time_idx\"] = (train_tft[\"영업일자\"] - train_tft[\"영업일자\"].min()).dt.days.astype(int)\n",
    "for c in [\"store_id\", \"item_id\", \"pair_id\"]:\n",
    "    train_tft[c] = train_tft[c].astype(str)\n",
    "\n",
    "static_categoricals = [\"store_id\", \"item_id\", \"pair_id\"]\n",
    "\n",
    "time_varying_known_reals = [\n",
    "    \"time_idx\",\n",
    "    \"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_weekend\",\"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "]\n",
    "\n",
    "time_varying_unknown_reals = [\"매출수량\"] + [f\"roll_mean_{w}\" for w in ROLL_WINS]\n",
    "\n",
    "for c in time_varying_known_reals:\n",
    "    if c != \"time_idx\":\n",
    "        train_tft[c] = pd.to_numeric(train_tft[c], errors=\"coerce\").astype(\"float32\")\n",
    "train_tft[\"time_idx\"] = pd.to_numeric(train_tft[\"time_idx\"], errors=\"coerce\").astype(\"int64\")\n",
    "for c in [f\"roll_mean_{w}\" for w in ROLL_WINS] + [\"매출수량\"]:\n",
    "    train_tft[c] = pd.to_numeric(train_tft[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "training_cutoff = train_tft[\"time_idx\"].max() - PRED_LEN\n",
    "\n",
    "# --- Modified: added GroupNormalizer ---\n",
    "tft_dataset = TimeSeriesDataSet(\n",
    "    train_tft[train_tft.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"매출수량\",\n",
    "    group_ids=[\"pair_id\"],\n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_encoder_length=ENC_LEN,\n",
    "    min_prediction_length=PRED_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    static_categoricals=static_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"pair_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(tft_dataset, train_tft, predict=True, stop_randomization=True)\n",
    "train_loader = tft_dataset.to_dataloader(train=True, batch_size=256, num_workers=2)\n",
    "val_loader   = validation.to_dataloader(train=False, batch_size=256, num_workers=2)\n",
    "\n",
    "print('[Define Dataset] dataset ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================",
    "# XGBoost 학습 (h=1..7) -- Added",
    "# ==============================================================",
    "import xgboost as xgb",
    "if xgb_choice:",
    "    xgb_models = joblib.load(os.path.join(MODEL_DIR, xgb_choice))",
    "    print(f\"[XGBoost] Loaded {xgb_choice}\")",
    "else:",
    "    xgb_models = {}",
    "    for h in range(1, PRED_LEN + 1):",
    "        Xh, yh = Xy_h[h]",
    "        if len(Xh) == 0:",
    "            print(f\"[XGBoost][H{h}] 학습 데이터가 없습니다. 건너뜁니다.\")",
    "            continue",
    "        X_tr, y_tr, X_va, y_va = split_train_valid(Xh, yh, valid_ratio=0.1)",
    "        print(f\"[XGBoost][H{h}] train={len(X_tr):,}  valid={len(X_va):,}\")",
    "        model = xgb.XGBRegressor(",
    "            max_depth=8,",
    "            learning_rate=0.05,",
    "            n_estimators=2000,",
    "            subsample=0.8,",
    "            colsample_bytree=0.8,",
    "            reg_lambda=5,",
    "            objective='reg:squarederror',",
    "            tree_method='hist',",
    "            random_state=42,",
    "            enable_categorical=True,",
    "        )",
    "        model.fit(X_tr, y_tr,",
    "                  eval_set=[(X_va, y_va)],",
    "                  verbose=200,",
    "                  early_stopping_rounds=100)",
    "        xgb_models[h] = model",
    "        gc.collect()",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")",
    "    xgb_path = os.path.join(MODEL_DIR, f'xgb_model_{ts}.json')",
    "    joblib.dump(xgb_models, xgb_path)",
    "    print('[XGBoost] Saved model to', xgb_path)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================",
    "# CatBoost 학습 (h=1..7)",
    "# ==============================================================",
    "if cat_choice:",
    "    cat_models = joblib.load(os.path.join(MODEL_DIR, cat_choice))",
    "    print(f\"[CatBoost] Loaded {cat_choice}\")",
    "else:",
    "    if \"split_train_valid\" not in globals():",
    "        def split_train_valid(X: pd.DataFrame, y: pd.Series, valid_ratio=0.1):",
    "            n = len(X); k = int(n * (1 - valid_ratio))",
    "            return (X.iloc[:k].reset_index(drop=True), y.iloc[:k].reset_index(drop=True),",
    "                    X.iloc[k:].reset_index(drop=True), y.iloc[k:].reset_index(drop=True))",
    "    if \"cat_col_indices\" not in globals():",
    "        def cat_col_indices(cols):",
    "            return [i for i, c in enumerate(cols) if c in cat_features_cols]",
    "    cat_models = {}",
    "    for h in range(1, PRED_LEN + 1):",
    "        Xh, yh = Xy_h[h]",
    "        if len(Xh) == 0:",
    "            print(f\"[CatBoost][H{h}] 학습 데이터가 없습니다. 건너뜁니다.\")",
    "            continue",
    "        cat_idx = cat_col_indices(Xh.columns)",
    "        X_tr, y_tr, X_va, y_va = split_train_valid(Xh, yh, valid_ratio=0.1)",
    "        print(f\"[CatBoost][H{h}] train={len(X_tr):,}  valid={len(X_va):,}\")",
    "        train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)",
    "        valid_pool = Pool(X_va, y_va, cat_features=cat_idx)",
    "        cb = CatBoostRegressor(**CAT_PARAMS)",
    "        cb.fit(",
    "            train_pool,",
    "            eval_set=valid_pool,",
    "            use_best_model=True,",
    "            verbose=200",
    "        )",
    "        cat_models[h] = cb",
    "    gc.collect()",
    "    if torch.cuda.is_available():",
    "        torch.cuda.empty_cache()",
    "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")",
    "    cat_path = os.path.join(MODEL_DIR, f'catboost_model_{ts}.cbm')",
    "    joblib.dump(cat_models, cat_path)",
    "    print('[CatBoost] Saved model to', cat_path)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7560266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================",
    "# TFT-mini 학습 (dropout 최적화 포함)",
    "# ==============================================================",
    "import optuna",
    "if tft_choice:",
    "    tft_model = TemporalFusionTransformer.load_from_checkpoint(os.path.join(MODEL_DIR, tft_choice))",
    "    print(f\"[TFT] Loaded {tft_choice}\")",
    "else:",
    "    # --- Modified: dropout optimization with Optuna ---",
    "    def tft_objective(trial):",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)",
    "        model = TemporalFusionTransformer.from_dataset(",
    "            tft_dataset,",
    "            loss=RMSE(),",
    "            log_interval=200,",
    "            reduce_on_plateau_patience=4,",
    "            dropout=dropout,",
    "            weight_decay=1e-2,",
    "            **TFT_PARAMS",
    "        )",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')",
    "        trainer = L.Trainer(",
    "            max_epochs=30,",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu',",
    "            devices=1,",
    "            precision='bf16-mixed' if torch.cuda.is_bf16_supported() else 32,",
    "            gradient_clip_val=0.1,",
    "            callbacks=[early_stop],",
    "            logger=False,",
    "            enable_progress_bar=False,",
    "            limit_val_batches=1.0,",
    "            num_sanity_val_steps=0,",
    "        )",
    "        trainer.fit(model, train_loader, val_loader)",
    "        return trainer.callback_metrics['val_loss'].item()",
    "    study = optuna.create_study(direction='minimize')",
    "    study.optimize(tft_objective, n_trials=10)",
    "    best_dropout = study.best_params['dropout']",
    "    print('Best dropout:', best_dropout)",
    "    # --- Train final model with optimal dropout ---",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=6, mode='min')",
    "    lr_logger  = LearningRateMonitor(logging_interval='epoch')",
    "    checkpoint = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min')",
    "    logger     = CSVLogger('tft_logs', name='catboost_tft')",
    "    precision = 'bf16-mixed' if torch.cuda.is_bf16_supported() else 32",
    "    print(f'[Precision] Using {precision}')",
    "    tft_model = TemporalFusionTransformer.from_dataset(",
    "        tft_dataset,",
    "        loss=RMSE(),",
    "        log_interval=200,",
    "        reduce_on_plateau_patience=4,",
    "        dropout=best_dropout,",
    "        weight_decay=1e-2,",
    "        **TFT_PARAMS",
    "    )",
    "    trainer = L.Trainer(",
    "        max_epochs=30,",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',",
    "        devices=1,",
    "        precision=precision,",
    "        gradient_clip_val=0.1,",
    "        callbacks=[early_stop, lr_logger, checkpoint],",
    "        logger=logger,",
    "        enable_progress_bar=True,",
    "        limit_val_batches=1.0,",
    "        num_sanity_val_steps=0,",
    "    )",
    "    trainer.fit(tft_model, train_dataloaders=train_loader, val_dataloaders=val_loader)",
    "    if checkpoint.best_model_path:",
    "        tft_model = TemporalFusionTransformer.load_from_checkpoint(checkpoint.best_model_path)",
    "        ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")",
    "        tft_path = os.path.join(MODEL_DIR, f'tft_model_{ts}.ckpt')",
    "        shutil.copy(checkpoint.best_model_path, tft_path)",
    "        print('[TFT] Saved model to', tft_path)",
    "    else:",
    "        print('[TFT] Warning: no best checkpoint found.')",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1066b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Prediction Functions (CatBoost, TFT-mini, XGBoost) -- Modified\n",
    "# ==============================================================\n",
    "\n",
    "def convert_to_submission_format(pred_df: pd.DataFrame, sample_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = sample_df.copy()\n",
    "    wide = pred_df.pivot(index='영업일자', columns='영업장명_메뉴명', values='매출수량')\n",
    "    for r in range(len(out)):\n",
    "        date_key = out.at[r, '영업일자']\n",
    "        if date_key in wide.index:\n",
    "            for c in out.columns:\n",
    "                if c == '영업일자':\n",
    "                    continue\n",
    "                if c in wide.columns:\n",
    "                    val = wide.at[date_key, c]\n",
    "                    out.at[r, c] = 0 if pd.isna(val) else max(float(val), 0.0)\n",
    "    for c in out.columns:\n",
    "        if c == '영업일자':\n",
    "            continue\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce').fillna(0).clip(lower=0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_test(df_test_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_test_raw.copy()\n",
    "    df['영업장명_메뉴명'] = df['영업장명_메뉴명'].astype(str)\n",
    "    df['업장명'] = df['영업장명_메뉴명'].str.split('_', n=1).str[0]\n",
    "    df['메뉴명'] = df['영업장명_메뉴명'].str.split('_', n=1).str[1].fillna('NA')\n",
    "    df['store_id'] = df['업장명'].map(store2id).fillna(-1).astype(int).astype(str)\n",
    "    df['item_id']  = df['메뉴명'].map(item2id).fillna(-1).astype(int).astype(str)\n",
    "    df['pair_id']  = (df['업장명'] + '###' + df['메뉴명']).map(pair2id).fillna(-1).astype(int).astype(str)\n",
    "    df = df.sort_values(['영업장명_메뉴명','영업일자']).reset_index(drop=True)\n",
    "    df = add_domain_features(df)\n",
    "    for w in ROLL_WINS:\n",
    "        df[f'roll_mean_{w}'] = (\n",
    "            df.groupby('영업장명_메뉴명')['매출수량']\n",
    "              .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# --- CatBoost prediction ---\n",
    "def predict_catboost(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds=[]\n",
    "    for pair, sub in df_28.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자').reset_index(drop=True)\n",
    "        history=sub.iloc[-ENC_LEN:].copy()\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            fut_date = history['영업일자'].iloc[-1] + timedelta(days=1)\n",
    "            fut_row=pd.DataFrame([{ '영업일자':fut_date,\n",
    "                                    '영업장명_메뉴명':pair,\n",
    "                                    '업장명':history['업장명'].iloc[-1],\n",
    "                                    '메뉴명':history['메뉴명'].iloc[-1],\n",
    "                                    'store_id':history['store_id'].iloc[-1],\n",
    "                                    'item_id':history['item_id'].iloc[-1],\n",
    "                                    'pair_id':history['pair_id'].iloc[-1],\n",
    "                                    '매출수량':0 }])\n",
    "            fut_row=add_domain_features(fut_row)\n",
    "            tmp_hist=pd.concat([history, fut_row], ignore_index=True)\n",
    "            for w in ROLL_WINS:\n",
    "                tmp_hist[f'roll_mean_{w}'] = (\n",
    "                    tmp_hist.groupby('영업장명_메뉴명')['매출수량']\n",
    "                           .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            fut_row=tmp_hist.iloc[[-1]]\n",
    "            X=fut_row[cat_features_cols + [f'roll_mean_{w}' for w in ROLL_WINS]].copy()\n",
    "            fut_feats=fut_row[known_future_cols].copy(); fut_feats.columns=[c+f'_H{h}' for c in fut_feats.columns]\n",
    "            X=pd.concat([X.reset_index(drop=True), fut_feats.reset_index(drop=True)], axis=1)\n",
    "            model=cat_models[h]; cat_idx=[i for i,c in enumerate(X.columns) if c in cat_features_cols]\n",
    "            yhat=float(model.predict(Pool(X, cat_features=cat_idx))[0])\n",
    "            yhat=max(0.0, yhat)\n",
    "            preds.append({'영업장명_메뉴명':pair,'h':h,'pred_cat':yhat})\n",
    "            fut_row.loc[:, '매출수량']=yhat\n",
    "            history=pd.concat([history, fut_row], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "# --- XGBoost prediction ---\n",
    "def predict_xgboost(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds=[]\n",
    "    for pair, sub in df_28.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자').reset_index(drop=True)\n",
    "        history=sub.iloc[-ENC_LEN:].copy()\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            fut_date=history['영업일자'].iloc[-1] + timedelta(days=1)\n",
    "            fut_row=pd.DataFrame([{ '영업일자':fut_date,\n",
    "                                    '영업장명_메뉴명':pair,\n",
    "                                    '업장명':history['업장명'].iloc[-1],\n",
    "                                    '메뉴명':history['메뉴명'].iloc[-1],\n",
    "                                    'store_id':history['store_id'].iloc[-1],\n",
    "                                    'item_id':history['item_id'].iloc[-1],\n",
    "                                    'pair_id':history['pair_id'].iloc[-1],\n",
    "                                    '매출수량':0 }])\n",
    "            fut_row=add_domain_features(fut_row)\n",
    "            tmp_hist=pd.concat([history, fut_row], ignore_index=True)\n",
    "            for w in ROLL_WINS:\n",
    "                tmp_hist[f'roll_mean_{w}'] = (\n",
    "                    tmp_hist.groupby('영업장명_메뉴명')['매출수량']\n",
    "                           .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            fut_row=tmp_hist.iloc[[-1]]\n",
    "            X=fut_row[cat_features_cols + [f'roll_mean_{w}' for w in ROLL_WINS]].copy()\n",
    "            fut_feats=fut_row[known_future_cols].copy(); fut_feats.columns=[c+f'_H{h}' for c in fut_feats.columns]\n",
    "            X=pd.concat([X.reset_index(drop=True), fut_feats.reset_index(drop=True)], axis=1)\n",
    "            model=xgb_models[h]\n",
    "            yhat=float(model.predict(X)[0])\n",
    "            yhat=max(0.0, yhat)\n",
    "            preds.append({'영업장명_메뉴명':pair,'h':h,'pred_xgb':yhat})\n",
    "            fut_row.loc[:, '매출수량']=yhat\n",
    "            history=pd.concat([history, fut_row], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "# --- TFT prediction ---\n",
    "def predict_tft(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp=df_28.copy()\n",
    "    tmp['time_idx']=(tmp['영업일자'] - train['영업일자'].min()).dt.days.astype(int)\n",
    "    rows=[]\n",
    "    for pair, sub in tmp.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자')\n",
    "        last_date=sub['영업일자'].iloc[-1]\n",
    "        pid=sub['pair_id'].iloc[-1]; sid=sub['store_id'].iloc[-1]; iid=sub['item_id'].iloc[-1]\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            d=last_date + timedelta(days=h)\n",
    "            rows.append({'영업장명_메뉴명':pair,'영업일자':d,'pair_id':pid,'store_id':sid,'item_id':iid})\n",
    "    fut=pd.DataFrame(rows)\n",
    "    fut=add_domain_features(fut.assign(매출수량=0))\n",
    "    for w in ROLL_WINS:\n",
    "        fut[f'roll_mean_{w}']=(\n",
    "            fut.groupby('영업장명_메뉴명')['매출수량']\n",
    "               .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "               .fillna(0)\n",
    "        )\n",
    "    fut['time_idx']=(fut['영업일자'] - train['영업일자'].min()).dt.days.astype(int)\n",
    "    enc_dec=pd.concat([tmp, fut], ignore_index=True)\n",
    "    enc_dec=enc_dec.fillna(0)\n",
    "    predict_ds=TimeSeriesDataSet.from_dataset(tft_dataset, enc_dec, predict=True, stop_randomization=True)\n",
    "    predict_loader=predict_ds.to_dataloader(train=False, batch_size=256, num_workers=2)\n",
    "    yhat=tft_model.predict(predict_loader, mode='prediction')\n",
    "    if isinstance(yhat, tuple): yhat=yhat[0]\n",
    "    fut_sorted=fut.sort_values(['pair_id','영업일자']).reset_index(drop=True)\n",
    "    series_ids=fut_sorted['pair_id'].unique().tolist()\n",
    "    out=[]\n",
    "    for i,pid in enumerate(series_ids):\n",
    "        pair_name=df_28.loc[df_28['pair_id']==pid, '영업장명_메뉴명'].iloc[0]\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            out.append({'영업장명_메뉴명':pair_name,'h':h,'pred_tft':float(max(0.0, yhat[i, h-1].item()))})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# --- helper to get predictions from all models ---\n",
    "def predict_all_models(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat_pred=predict_catboost(df_28)\n",
    "    tft_pred=predict_tft(df_28)\n",
    "    xgb_pred=predict_xgboost(df_28)\n",
    "    pred=cat_pred.merge(tft_pred, on=['영업장명_메뉴명','h'], how='outer')                 .merge(xgb_pred, on=['영업장명_메뉴명','h'], how='outer').fillna(0.0)\n",
    "    return pred\n",
    "\n",
    "# --- ensemble prediction using weights ---\n",
    "def predict_ensemble_for_test_file(test_file_path: str, weights: dict) -> pd.DataFrame:\n",
    "    df_raw=pd.read_csv(test_file_path, parse_dates=['영업일자'])\n",
    "    df_28=prepare_test(df_raw)\n",
    "    pred=predict_all_models(df_28)\n",
    "    pred['pred_ens'] = (weights['w_cat']*pred.get('pred_cat',0) + weights['w_tft']*pred.get('pred_tft',0) + weights['w_xgb']*pred.get('pred_xgb',0))\n",
    "    pred['file']=os.path.basename(test_file_path)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Ensemble Weight Optimization (Optuna) -- Added\n",
    "# ==============================================================\n",
    "\n",
    "def build_validation_context(train_df):\n",
    "    contexts=[]\n",
    "    targets=[]\n",
    "    for pair, sub in train_df.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자')\n",
    "        if len(sub) < ENC_LEN + PRED_LEN:\n",
    "            continue\n",
    "        hist=sub.iloc[-(ENC_LEN+PRED_LEN):-PRED_LEN].copy()\n",
    "        fut=sub.iloc[-PRED_LEN:]['매출수량'].tolist()\n",
    "        hist['영업장명_메뉴명']=pair\n",
    "        contexts.append(hist)\n",
    "        for h,val in enumerate(fut,1):\n",
    "            targets.append({'영업장명_메뉴명':pair,'h':h,'y_true':float(val)})\n",
    "    ctx=pd.concat(contexts).reset_index(drop=True)\n",
    "    tgt=pd.DataFrame(targets)\n",
    "    return ctx,tgt\n",
    "\n",
    "val_ctx,val_tgt=build_validation_context(train)\n",
    "val_pred=predict_all_models(val_ctx).merge(val_tgt, on=['영업장명_메뉴명','h'])\n",
    "val_pred=val_pred.fillna(0.0)\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(2*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+1e-8))\n",
    "\n",
    "\n",
    "def weight_objective(trial):\n",
    "    w_cat=trial.suggest_float('w_cat',0,1)\n",
    "    w_tft=trial.suggest_float('w_tft',0,1-w_cat)\n",
    "    w_xgb=1 - w_cat - w_tft\n",
    "    if w_xgb < 0:\n",
    "        raise optuna.TrialPruned()\n",
    "    y_hat=w_cat*val_pred['pred_cat'] + w_tft*val_pred['pred_tft'] + w_xgb*val_pred['pred_xgb']\n",
    "    return smape(val_pred['y_true'], y_hat)\n",
    "\n",
    "study_w=optuna.create_study(direction='minimize')\n",
    "study_w.optimize(weight_objective, n_trials=50)\n",
    "best_weights=study_w.best_params\n",
    "best_weights['w_xgb']=1 - best_weights['w_cat'] - best_weights['w_tft']\n",
    "print('Best weights:', best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Prediction 실행\n",
    "# -------------------------------\n",
    "all_preds = []\n",
    "for test_path in sorted(test_paths):\n",
    "    print(f'[Predict] {test_path}')\n",
    "    all_preds.append(predict_ensemble_for_test_file(test_path, best_weights))\n",
    "all_preds = pd.concat(all_preds, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888817bf",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8249e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime",
    "sample_submission = pd.read_csv(sample_path)",
    "submission_df = convert_preds_to_submission(all_preds, sample_submission)",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")",
    "output_path = os.path.join(SUB_DIR, f'submission_{ts}.csv')",
    "submission_df.to_csv(output_path, index=False)",
    "weights_path = os.path.join(MODEL_DIR, f'ensemble_weights_{ts}.json')",
    "with open(weights_path, 'w') as f:",
    "    json.dump(best_weights, f)",
    "print('Saved submission to', output_path)",
    "print('Saved ensemble weights to', weights_path)",
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}