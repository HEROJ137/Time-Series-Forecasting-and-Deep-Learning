{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2e3948",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost pytorch-lightning pytorch-forecasting optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, gc, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "import optuna\n",
    "import glob, re\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc6a89",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed & Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 설정\n",
    "ENC_LEN  = 28\n",
    "PRED_LEN = 7\n",
    "ROLL_WINS = [7, 14, 28]\n",
    "\n",
    "# CatBoost 하이퍼파라미터\n",
    "CAT_PARAMS = dict(\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    iterations=2000,\n",
    "    loss_function=\"RMSE\",\n",
    "    l2_leaf_reg=5,\n",
    "    random_seed=42,\n",
    "    verbose=200,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    ")\n",
    "\n",
    "# TFT-mini 하이퍼파라미터 (dropout tuned later)\n",
    "TFT_PARAMS = dict(\n",
    "    learning_rate=2e-3,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=2,\n",
    "    hidden_continuous_size=32,\n",
    "    lstm_layers=1,\n",
    ")\n",
    "\n",
    "# 앙상블 가중치\n",
    "W_CAT = 0.4\n",
    "W_TFT = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3aef3",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753047b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/open'\n",
    "file_path = BASE_DIR\n",
    "train_path  = os.path.join(BASE_DIR,'train', \"train.csv\")\n",
    "sample_path = os.path.join(BASE_DIR, \"sample_submission.csv\")\n",
    "test_paths  = [os.path.join(BASE_DIR,'test', f\"TEST_0{i}.csv\") for i in range(10)]\n",
    "\n",
    "train  = pd.read_csv(train_path,  parse_dates=[\"영업일자\"])\n",
    "sample = pd.read_csv(sample_path)\n",
    "tests  = {f\"TEST_0{i}\": pd.read_csv(p, parse_dates=[\"영업일자\"]) for i, p in enumerate(test_paths)}\n",
    "\n",
    "# 라벨 고정 및 키 분해\n",
    "train[\"매출수량\"] = pd.to_numeric(train[\"매출수량\"], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "train[\"영업장명_메뉴명\"] = train[\"영업장명_메뉴명\"].astype(str)\n",
    "train[\"업장명\"] = train[\"영업장명_메뉴명\"].str.split(\"_\", n=1).str[0]\n",
    "train[\"메뉴명\"] = train[\"영업장명_메뉴명\"].str.split(\"_\", n=1).str[1].fillna(\"NA\")\n",
    "train = train.sort_values([\"영업장명_메뉴명\",\"영업일자\"]).reset_index(drop=True)\n",
    "\n",
    "# ID 매핑\n",
    "def make_id_map(s: pd.Series):\n",
    "    uniq = s.astype(str).unique().tolist()\n",
    "    return {k: i for i, k in enumerate(uniq)}\n",
    "\n",
    "store2id = make_id_map(train[\"업장명\"])\n",
    "item2id  = make_id_map(train[\"메뉴명\"])\n",
    "pair2id  = make_id_map(train[\"업장명\"] + \"###\" + train[\"메뉴명\"])\n",
    "\n",
    "train[\"store_id\"] = train[\"업장명\"].map(store2id).astype(int)\n",
    "train[\"item_id\"]  = train[\"메뉴명\"].map(item2id).astype(int)\n",
    "train[\"pair_id\"]  = (train[\"업장명\"] + \"###\" + train[\"메뉴명\"]).map(pair2id).astype(int)\n",
    "\n",
    "# 공휴일/이벤트 정의\n",
    "KOREA_HOLIDAYS = set([\n",
    "    # ----- 2023 -----\n",
    "    '2023-01-01',  # 신정\n",
    "    '2023-01-21',  # 설날 연휴 시작\n",
    "    '2023-01-22',  # 설날\n",
    "    '2023-01-23',  # 설날 연휴\n",
    "    '2023-01-24',  # 대체공휴일\n",
    "    '2023-03-01',  # 삼일절\n",
    "    '2023-05-05',  # 어린이날\n",
    "    '2023-05-27',  # 부처님오신날\n",
    "    '2023-05-29',  # 부처님오신날 대체휴일\n",
    "    '2023-06-06',  # 현충일\n",
    "    '2023-08-15',  # 광복절\n",
    "    '2023-09-28',  # 추석 연휴 시작\n",
    "    '2023-09-29',  # 추석\n",
    "    '2023-09-30',  # 추석 연휴\n",
    "    '2023-10-02',  # 임시공휴일\n",
    "    '2023-10-03',  # 개천절\n",
    "    '2023-10-09',  # 한글날\n",
    "    '2023-12-25',  # 크리스마스\n",
    "    # ----- 2024 -----\n",
    "    '2024-01-01',  # 신정\n",
    "    '2024-02-09',  # 설날 연휴 시작\n",
    "    '2024-02-10',  # 설날\n",
    "    '2024-02-11',  # 설날 연휴\n",
    "    '2024-02-12',  # 대체공휴일\n",
    "    '2024-03-01',  # 삼일절\n",
    "    '2024-04-10',  # 제22대 국회의원 선거\n",
    "    '2024-05-05',  # 어린이날\n",
    "    '2024-05-06',  # 어린이날 대체휴일\n",
    "    '2024-05-15',  # 석가탄신일\n",
    "    '2024-06-06',  # 현충일\n",
    "    '2024-08-15',  # 광복절\n",
    "    '2024-09-16',  # 추석 연휴 시작\n",
    "    '2024-09-17',  # 추석\n",
    "    '2024-09-18',  # 추석 연휴\n",
    "    '2024-10-03',  # 개천절\n",
    "    '2024-10-09',  # 한글날\n",
    "    '2024-12-25',  # 크리스마스\n",
    "])\n",
    "\n",
    "EVENTS_GLOBAL = set([\n",
    "\n",
    "])\n",
    "\n",
    "EVENTS_TARGETED = {\n",
    "\n",
    "}\n",
    "\n",
    "def expand_dates(dates, days=3):\n",
    "    if not dates:\n",
    "        return pd.DataFrame(columns=[\"영업일자\",\"near_flag\"])\n",
    "    base = pd.to_datetime(sorted(pd.to_datetime(list(dates)))).normalize()\n",
    "    offs = pd.to_timedelta(np.arange(-days, days+1), unit=\"D\")\n",
    "    expanded = (base.values[:, None] + offs.values[None, :]).ravel()\n",
    "    out = pd.DataFrame({\"영업일자\": pd.to_datetime(expanded).normalize(), \"near_flag\": 1})\n",
    "    return out.drop_duplicates()\n",
    "\n",
    "df_near_glob = expand_dates(EVENTS_GLOBAL, days=3)\n",
    "near_glob_map = df_near_glob.set_index(\"영업일자\")[\"near_flag\"] if len(df_near_glob) else pd.Series(dtype=\"int\")\n",
    "\n",
    "def add_domain_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dt = pd.to_datetime(df[\"영업일자\"])\n",
    "    df[\"dow\"] = dt.dt.weekday\n",
    "    df[\"month\"] = dt.dt.month\n",
    "    df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "    df[\"dow_sin\"] = np.sin(2*np.pi*df[\"dow\"]/7.0).astype(np.float32)\n",
    "    df[\"dow_cos\"] = np.cos(2*np.pi*df[\"dow\"]/7.0).astype(np.float32)\n",
    "    df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12.0).astype(np.float32)\n",
    "    df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12.0).astype(np.float32)\n",
    "\n",
    "    df[\"is_spring\"] = df[\"month\"].isin([3,4,5]).astype(int)\n",
    "    df[\"is_summer\"] = df[\"month\"].isin([6,7,8]).astype(int)\n",
    "    df[\"is_fall\"]   = df[\"month\"].isin([9,10,11]).astype(int)\n",
    "    df[\"is_winter\"] = df[\"month\"].isin([12,1,2]).astype(int)\n",
    "\n",
    "    df[\"is_peak_summer\"] = df[\"month\"].isin([7,8]).astype(int)\n",
    "    df[\"is_peak_winter\"] = df[\"month\"].isin([12,1,2]).astype(int)\n",
    "\n",
    "    date_str = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "    date_key = dt.dt.normalize()\n",
    "    df[\"is_holiday\"] = date_str.isin(KOREA_HOLIDAYS).astype(int)\n",
    "    prev_date = (dt - pd.Timedelta(days=1)).dt.strftime(\"%Y-%m-%d\")\n",
    "    next_date = (dt + pd.Timedelta(days=1)).dt.strftime(\"%Y-%m-%d\")\n",
    "    df[\"before_holiday\"] = prev_date.isin(KOREA_HOLIDAYS).astype(int)\n",
    "    df[\"after_holiday\"]  = next_date.isin(KOREA_HOLIDAYS).astype(int)\n",
    "\n",
    "    if len(KOREA_HOLIDAYS) > 0:\n",
    "        holi_sorted = pd.to_datetime(sorted(list(KOREA_HOLIDAYS))).normalize()\n",
    "        df_h = pd.DataFrame({\"d\": holi_sorted})\n",
    "        grp = (df_h[\"d\"].diff().dt.days.ne(1)).cumsum()\n",
    "        runlen = df_h.groupby(grp)[\"d\"].transform(\"size\")\n",
    "        run_map = pd.Series(runlen.values, index=df_h[\"d\"].values)\n",
    "        df[\"is_holiday_run\"] = date_key.map(run_map).fillna(0).astype(np.int16)\n",
    "    else:\n",
    "        df[\"is_holiday_run\"] = 0\n",
    "\n",
    "    start_su = pd.to_datetime(dt.dt.year.astype(str) + \"-07-15\")\n",
    "    end_su   = pd.to_datetime(dt.dt.year.astype(str) + \"-08-31\")\n",
    "    df[\"is_summer_vac\"] = ((dt >= start_su) & (dt <= end_su)).astype(int)\n",
    "\n",
    "    start_w1 = pd.to_datetime(dt.dt.year.astype(str) + \"-12-20\")\n",
    "    end_w1   = pd.to_datetime(dt.dt.year.astype(str) + \"-12-31\")\n",
    "    start_w2 = pd.to_datetime((dt.dt.year-1).astype(str) + \"-12-20\")\n",
    "    end_w2   = pd.to_datetime(dt.dt.year.astype(str) + \"-02-28\")\n",
    "    df[\"is_winter_vac\"] = (((dt >= start_w1) & (dt <= end_w1)) | ((dt >= start_w2) & (dt <= end_w2))).astype(int)\n",
    "\n",
    "    df[\"EVENT_SF_SZN\"]      = df[\"month\"].isin([3,4,5,9,10,11]).astype(int)\n",
    "    df[\"EVENT_SUMMER_SZN\"]  = df[\"month\"].isin([6,7,8]).astype(int)\n",
    "    df[\"EVENT_WINTER_SZN\"]  = df[\"month\"].isin([12,1,2]).astype(int)\n",
    "\n",
    "    df[\"is_event_global\"] = date_str.isin(EVENTS_GLOBAL).astype(int)\n",
    "    df[\"near_event_global\"] = 0 if near_glob_map.empty else date_key.map(near_glob_map).fillna(0).astype(\"int8\")\n",
    "\n",
    "    df[\"is_event_target\"] = 0\n",
    "    df[\"near_event_target\"] = 0\n",
    "    if \"업장명\" in df.columns and \"메뉴명\" in df.columns:\n",
    "        for (s_name, i_name), dates in EVENTS_TARGETED.items():\n",
    "            if not dates:\n",
    "                continue\n",
    "            m = (df[\"업장명\"].eq(s_name)) & (df[\"메뉴명\"].eq(i_name))\n",
    "            df.loc[m, \"is_event_target\"] = df.loc[m, \"영업일자\"].dt.strftime(\"%Y-%m-%d\").isin(set(dates)).astype(int)\n",
    "            df_near = expand_dates(dates, days=3)\n",
    "            if len(df_near):\n",
    "                mapper = df_near.set_index(\"영업일자\")[\"near_flag\"]\n",
    "                df.loc[m, \"near_event_target\"] = df.loc[m, \"영업일자\"].dt.normalize().map(mapper).fillna(0).astype(\"int8\")\n",
    "\n",
    "    return df\n",
    "\n",
    "train = add_domain_features(train)\n",
    "\n",
    "# 롤링 통계(과거 기반)\n",
    "for w in ROLL_WINS:\n",
    "    train[f\"roll_mean_{w}\"] = train.groupby(\"영업장명_메뉴명\")[\"매출수량\"].transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "\n",
    "# CatBoost용 피처 구성\n",
    "cat_features_cols = [\"업장명\",\"메뉴명\"]\n",
    "base_num_features = [\n",
    "    \"is_weekend\",\"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "] + [f\"roll_mean_{w}\" for w in ROLL_WINS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3afdf",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff71d0f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# (1) 미래에 '알고 있는' 달력/이벤트 피처 목록\n",
    "# --------------------------------------------------------------\n",
    "known_future_cols = [\n",
    "    \"dow\",\"month\",\"is_weekend\",\"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# (2) CatBoost 학습용: shift 벡터화 + fragmentation 방지\n",
    "# --------------------------------------------------------------\n",
    "train_sorted = train.sort_values([\"영업장명_메뉴명\",\"영업일자\"]).reset_index(drop=True)\n",
    "g = train_sorted.groupby(\"영업장명_메뉴명\", sort=False)\n",
    "train_sorted[\"hist_ok\"] = g.cumcount() >= (ENC_LEN - 1)\n",
    "\n",
    "base_for_shift = [\"매출수량\"] + known_future_cols\n",
    "shift_blocks = []\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    block = g[base_for_shift].shift(-h)\n",
    "    rename_map = {\"매출수량\": f\"y_H{h}\"}\n",
    "    rename_map.update({c: f\"{c}_H{h}\" for c in known_future_cols})\n",
    "    block = block.rename(columns=rename_map)\n",
    "    shift_blocks.append(block)\n",
    "\n",
    "train_shift = pd.concat(shift_blocks, axis=1)\n",
    "train_sorted = pd.concat([train_sorted, train_shift], axis=1).copy()\n",
    "\n",
    "def build_catboost_Xy_by_shift(df: pd.DataFrame, stride: int = 1):\n",
    "    Xy = {}\n",
    "    base_cols = cat_features_cols + [f\"roll_mean_{w}\" for w in ROLL_WINS]\n",
    "    for c in base_cols:\n",
    "        if c in cat_features_cols:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    for h in range(1, PRED_LEN + 1):\n",
    "        fut_cols_h = [f\"{c}_H{h}\" for c in known_future_cols]\n",
    "        target_col = f\"y_H{h}\"\n",
    "        mask = df[\"hist_ok\"] & df[target_col].notna()\n",
    "        use = df.loc[mask, base_cols + fut_cols_h + [target_col]].copy() if stride==1 else (\n",
    "            df.loc[\n",
    "                df[mask].groupby(\"영업장명_메뉴명\", sort=False).apply(lambda x: x.iloc[::stride]).reset_index(level=0, drop=True).index,\n",
    "                base_cols + fut_cols_h + [target_col]\n",
    "            ].copy()\n",
    "        )\n",
    "        for c in fut_cols_h:\n",
    "            use[c] = pd.to_numeric(use[c], errors=\"coerce\").astype(\"float32\")\n",
    "        use[target_col] = pd.to_numeric(use[target_col], errors=\"coerce\").astype(\"float32\")\n",
    "        X = use[base_cols + fut_cols_h].reset_index(drop=True)\n",
    "        y = use[target_col].reset_index(drop=True)\n",
    "        Xy[h] = (X, y)\n",
    "    return Xy\n",
    "\n",
    "Xy_h = build_catboost_Xy_by_shift(train_sorted, stride=1)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# (3) TFT용 데이터셋/로더\n",
    "# --------------------------------------------------------------\n",
    "train_tft = train.copy()\n",
    "train_tft[\"time_idx\"] = (train_tft[\"영업일자\"] - train_tft[\"영업일자\"].min()).dt.days.astype(int)\n",
    "for c in [\"store_id\", \"item_id\", \"pair_id\"]:\n",
    "    train_tft[c] = train_tft[c].astype(str)\n",
    "\n",
    "static_categoricals = [\"store_id\", \"item_id\", \"pair_id\"]\n",
    "\n",
    "time_varying_known_reals = [\n",
    "    \"time_idx\",\n",
    "    \"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_weekend\",\"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "]\n",
    "\n",
    "time_varying_unknown_reals = [\"매출수량\"] + [f\"roll_mean_{w}\" for w in ROLL_WINS]\n",
    "\n",
    "for c in time_varying_known_reals:\n",
    "    if c != \"time_idx\":\n",
    "        train_tft[c] = pd.to_numeric(train_tft[c], errors=\"coerce\").astype(\"float32\")\n",
    "train_tft[\"time_idx\"] = pd.to_numeric(train_tft[\"time_idx\"], errors=\"coerce\").astype(\"int64\")\n",
    "for c in [f\"roll_mean_{w}\" for w in ROLL_WINS] + [\"매출수량\"]:\n",
    "    train_tft[c] = pd.to_numeric(train_tft[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "training_cutoff = train_tft[\"time_idx\"].max() - PRED_LEN\n",
    "\n",
    "# --- Modified: added GroupNormalizer ---\n",
    "tft_dataset = TimeSeriesDataSet(\n",
    "    train_tft[train_tft.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"매출수량\",\n",
    "    group_ids=[\"pair_id\"],\n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_encoder_length=ENC_LEN,\n",
    "    min_prediction_length=PRED_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    static_categoricals=static_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"pair_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(tft_dataset, train_tft, predict=True, stop_randomization=True)\n",
    "train_loader = tft_dataset.to_dataloader(train=True, batch_size=256, num_workers=2)\n",
    "val_loader   = validation.to_dataloader(train=False, batch_size=256, num_workers=2)\n",
    "\n",
    "print('[Define Dataset] dataset ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# XGBoost 학습 (h=1..7) -- Added\n",
    "# ==============================================================\n",
    "import xgboost as xgb\n",
    "xgb_models = {}\n",
    "\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    Xh, yh = Xy_h[h]\n",
    "    if len(Xh) == 0:\n",
    "        print(f\"[XGBoost][H{h}] 학습 데이터가 없습니다. 건너뜁니다.\")\n",
    "        continue\n",
    "    X_tr, y_tr, X_va, y_va = split_train_valid(Xh, yh, valid_ratio=0.1)\n",
    "    print(f\"[XGBoost][H{h}] train={len(X_tr):,}  valid={len(X_va):,}\")\n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=2000,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=5,\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        enable_categorical=True,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr,\n",
    "              eval_set=[(X_va, y_va)],\n",
    "              verbose=200,\n",
    "              early_stopping_rounds=100)\n",
    "    xgb_models[h] = model\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# CatBoost 학습 (h=1..7)\n",
    "# ==============================================================\n",
    "if \"split_train_valid\" not in globals():\n",
    "    def split_train_valid(X: pd.DataFrame, y: pd.Series, valid_ratio=0.1):\n",
    "        n = len(X); k = int(n * (1 - valid_ratio))\n",
    "        return (X.iloc[:k].reset_index(drop=True), y.iloc[:k].reset_index(drop=True),\n",
    "                X.iloc[k:].reset_index(drop=True), y.iloc[k:].reset_index(drop=True))\n",
    "\n",
    "if \"cat_col_indices\" not in globals():\n",
    "    def cat_col_indices(cols):\n",
    "        return [i for i, c in enumerate(cols) if c in cat_features_cols]\n",
    "\n",
    "cat_models = {}\n",
    "\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    Xh, yh = Xy_h[h]\n",
    "    if len(Xh) == 0:\n",
    "        print(f\"[CatBoost][H{h}] 학습 데이터가 없습니다. 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 범주형 피처 인덱스\n",
    "    cat_idx = cat_col_indices(Xh.columns)\n",
    "\n",
    "    # 단순 시계열 분할(뒤쪽 10% 검증)\n",
    "    X_tr, y_tr, X_va, y_va = split_train_valid(Xh, yh, valid_ratio=0.1)\n",
    "    print(f\"[CatBoost][H{h}] train={len(X_tr):,}  valid={len(X_va):,}\")\n",
    "\n",
    "    train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
    "    valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n",
    "\n",
    "    # 모델 생성 및 학습\n",
    "    cb = CatBoostRegressor(**CAT_PARAMS)\n",
    "    cb.fit(\n",
    "        train_pool,\n",
    "        eval_set=valid_pool,\n",
    "        use_best_model=True,\n",
    "        verbose=200\n",
    "    )\n",
    "    cat_models[h] = cb\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7560266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# TFT-mini 학습 (dropout 최적화 포함)\n",
    "# ==============================================================\n",
    "\n",
    "# --- Modified: dropout optimization with Optuna ---\n",
    "def tft_objective(trial):\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        tft_dataset,\n",
    "        loss=RMSE(),\n",
    "        log_interval=200,\n",
    "        reduce_on_plateau_patience=4,\n",
    "        dropout=dropout,\n",
    "        weight_decay=1e-2,\n",
    "        **TFT_PARAMS\n",
    "    )\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=30,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        precision='bf16-mixed' if torch.cuda.is_bf16_supported() else 32,\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        logger=False,\n",
    "        enable_progress_bar=False,\n",
    "        limit_val_batches=1.0,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    return trainer.callback_metrics['val_loss'].item()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(tft_objective, n_trials=10)\n",
    "best_dropout = study.best_params['dropout']\n",
    "print('Best dropout:', best_dropout)\n",
    "\n",
    "# --- Train final model with optimal dropout ---\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
    "lr_logger  = LearningRateMonitor(logging_interval='epoch')\n",
    "checkpoint = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min')\n",
    "logger     = CSVLogger('tft_logs', name='catboost_tft')\n",
    "\n",
    "precision = 'bf16-mixed' if torch.cuda.is_bf16_supported() else 32\n",
    "print(f'[Precision] Using {precision}')\n",
    "\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    tft_dataset,\n",
    "    loss=RMSE(),\n",
    "    log_interval=200,\n",
    "    reduce_on_plateau_patience=4,\n",
    "    dropout=best_dropout,\n",
    "    weight_decay=1e-2,\n",
    "    **TFT_PARAMS\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop, lr_logger, checkpoint],\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True,\n",
    "    limit_val_batches=1.0,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(tft_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "if checkpoint.best_model_path:\n",
    "    tft_model = TemporalFusionTransformer.load_from_checkpoint(checkpoint.best_model_path)\n",
    "    print('[TFT] best checkpoint:', checkpoint.best_model_path)\n",
    "else:\n",
    "    print('[TFT] Warning: no best checkpoint found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1066b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Prediction Functions (CatBoost, TFT-mini, XGBoost) -- Modified\n",
    "# ==============================================================\n",
    "\n",
    "def convert_to_submission_format(pred_df: pd.DataFrame, sample_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = sample_df.copy()\n",
    "    wide = pred_df.pivot(index='영업일자', columns='영업장명_메뉴명', values='매출수량')\n",
    "    for r in range(len(out)):\n",
    "        date_key = out.at[r, '영업일자']\n",
    "        if date_key in wide.index:\n",
    "            for c in out.columns:\n",
    "                if c == '영업일자':\n",
    "                    continue\n",
    "                if c in wide.columns:\n",
    "                    val = wide.at[date_key, c]\n",
    "                    out.at[r, c] = 0 if pd.isna(val) else max(float(val), 0.0)\n",
    "    for c in out.columns:\n",
    "        if c == '영업일자':\n",
    "            continue\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce').fillna(0).clip(lower=0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_test(df_test_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_test_raw.copy()\n",
    "    df['영업장명_메뉴명'] = df['영업장명_메뉴명'].astype(str)\n",
    "    df['업장명'] = df['영업장명_메뉴명'].str.split('_', n=1).str[0]\n",
    "    df['메뉴명'] = df['영업장명_메뉴명'].str.split('_', n=1).str[1].fillna('NA')\n",
    "    df['store_id'] = df['업장명'].map(store2id).fillna(-1).astype(int).astype(str)\n",
    "    df['item_id']  = df['메뉴명'].map(item2id).fillna(-1).astype(int).astype(str)\n",
    "    df['pair_id']  = (df['업장명'] + '###' + df['메뉴명']).map(pair2id).fillna(-1).astype(int).astype(str)\n",
    "    df = df.sort_values(['영업장명_메뉴명','영업일자']).reset_index(drop=True)\n",
    "    df = add_domain_features(df)\n",
    "    for w in ROLL_WINS:\n",
    "        df[f'roll_mean_{w}'] = (\n",
    "            df.groupby('영업장명_메뉴명')['매출수량']\n",
    "              .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# --- CatBoost prediction ---\n",
    "def predict_catboost(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds=[]\n",
    "    for pair, sub in df_28.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자').reset_index(drop=True)\n",
    "        history=sub.iloc[-ENC_LEN:].copy()\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            fut_date = history['영업일자'].iloc[-1] + timedelta(days=1)\n",
    "            fut_row=pd.DataFrame([{ '영업일자':fut_date,\n",
    "                                    '영업장명_메뉴명':pair,\n",
    "                                    '업장명':history['업장명'].iloc[-1],\n",
    "                                    '메뉴명':history['메뉴명'].iloc[-1],\n",
    "                                    'store_id':history['store_id'].iloc[-1],\n",
    "                                    'item_id':history['item_id'].iloc[-1],\n",
    "                                    'pair_id':history['pair_id'].iloc[-1],\n",
    "                                    '매출수량':0 }])\n",
    "            fut_row=add_domain_features(fut_row)\n",
    "            tmp_hist=pd.concat([history, fut_row], ignore_index=True)\n",
    "            for w in ROLL_WINS:\n",
    "                tmp_hist[f'roll_mean_{w}'] = (\n",
    "                    tmp_hist.groupby('영업장명_메뉴명')['매출수량']\n",
    "                           .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            fut_row=tmp_hist.iloc[[-1]]\n",
    "            X=fut_row[cat_features_cols + [f'roll_mean_{w}' for w in ROLL_WINS]].copy()\n",
    "            fut_feats=fut_row[known_future_cols].copy(); fut_feats.columns=[c+f'_H{h}' for c in fut_feats.columns]\n",
    "            X=pd.concat([X.reset_index(drop=True), fut_feats.reset_index(drop=True)], axis=1)\n",
    "            model=cat_models[h]; cat_idx=[i for i,c in enumerate(X.columns) if c in cat_features_cols]\n",
    "            yhat=float(model.predict(Pool(X, cat_features=cat_idx))[0])\n",
    "            yhat=max(0.0, yhat)\n",
    "            preds.append({'영업장명_메뉴명':pair,'h':h,'pred_cat':yhat})\n",
    "            fut_row.loc[:, '매출수량']=yhat\n",
    "            history=pd.concat([history, fut_row], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "# --- XGBoost prediction ---\n",
    "def predict_xgboost(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds=[]\n",
    "    for pair, sub in df_28.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자').reset_index(drop=True)\n",
    "        history=sub.iloc[-ENC_LEN:].copy()\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            fut_date=history['영업일자'].iloc[-1] + timedelta(days=1)\n",
    "            fut_row=pd.DataFrame([{ '영업일자':fut_date,\n",
    "                                    '영업장명_메뉴명':pair,\n",
    "                                    '업장명':history['업장명'].iloc[-1],\n",
    "                                    '메뉴명':history['메뉴명'].iloc[-1],\n",
    "                                    'store_id':history['store_id'].iloc[-1],\n",
    "                                    'item_id':history['item_id'].iloc[-1],\n",
    "                                    'pair_id':history['pair_id'].iloc[-1],\n",
    "                                    '매출수량':0 }])\n",
    "            fut_row=add_domain_features(fut_row)\n",
    "            tmp_hist=pd.concat([history, fut_row], ignore_index=True)\n",
    "            for w in ROLL_WINS:\n",
    "                tmp_hist[f'roll_mean_{w}'] = (\n",
    "                    tmp_hist.groupby('영업장명_메뉴명')['매출수량']\n",
    "                           .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            fut_row=tmp_hist.iloc[[-1]]\n",
    "            X=fut_row[cat_features_cols + [f'roll_mean_{w}' for w in ROLL_WINS]].copy()\n",
    "            fut_feats=fut_row[known_future_cols].copy(); fut_feats.columns=[c+f'_H{h}' for c in fut_feats.columns]\n",
    "            X=pd.concat([X.reset_index(drop=True), fut_feats.reset_index(drop=True)], axis=1)\n",
    "            model=xgb_models[h]\n",
    "            yhat=float(model.predict(X)[0])\n",
    "            yhat=max(0.0, yhat)\n",
    "            preds.append({'영업장명_메뉴명':pair,'h':h,'pred_xgb':yhat})\n",
    "            fut_row.loc[:, '매출수량']=yhat\n",
    "            history=pd.concat([history, fut_row], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "# --- TFT prediction ---\n",
    "def predict_tft(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp=df_28.copy()\n",
    "    tmp['time_idx']=(tmp['영업일자'] - train['영업일자'].min()).dt.days.astype(int)\n",
    "    rows=[]\n",
    "    for pair, sub in tmp.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자')\n",
    "        last_date=sub['영업일자'].iloc[-1]\n",
    "        pid=sub['pair_id'].iloc[-1]; sid=sub['store_id'].iloc[-1]; iid=sub['item_id'].iloc[-1]\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            d=last_date + timedelta(days=h)\n",
    "            rows.append({'영업장명_메뉴명':pair,'영업일자':d,'pair_id':pid,'store_id':sid,'item_id':iid})\n",
    "    fut=pd.DataFrame(rows)\n",
    "    fut=add_domain_features(fut.assign(매출수량=0))\n",
    "    for w in ROLL_WINS:\n",
    "        fut[f'roll_mean_{w}']=(\n",
    "            fut.groupby('영업장명_메뉴명')['매출수량']\n",
    "               .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "               .fillna(0)\n",
    "        )\n",
    "    fut['time_idx']=(fut['영업일자'] - train['영업일자'].min()).dt.days.astype(int)\n",
    "    enc_dec=pd.concat([tmp, fut], ignore_index=True)\n",
    "    enc_dec=enc_dec.fillna(0)\n",
    "    predict_ds=TimeSeriesDataSet.from_dataset(tft_dataset, enc_dec, predict=True, stop_randomization=True)\n",
    "    predict_loader=predict_ds.to_dataloader(train=False, batch_size=256, num_workers=2)\n",
    "    yhat=tft_model.predict(predict_loader, mode='prediction')\n",
    "    if isinstance(yhat, tuple): yhat=yhat[0]\n",
    "    fut_sorted=fut.sort_values(['pair_id','영업일자']).reset_index(drop=True)\n",
    "    series_ids=fut_sorted['pair_id'].unique().tolist()\n",
    "    out=[]\n",
    "    for i,pid in enumerate(series_ids):\n",
    "        pair_name=df_28.loc[df_28['pair_id']==pid, '영업장명_메뉴명'].iloc[0]\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            out.append({'영업장명_메뉴명':pair_name,'h':h,'pred_tft':float(max(0.0, yhat[i, h-1].item()))})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# --- helper to get predictions from all models ---\n",
    "def predict_all_models(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat_pred=predict_catboost(df_28)\n",
    "    tft_pred=predict_tft(df_28)\n",
    "    xgb_pred=predict_xgboost(df_28)\n",
    "    pred=cat_pred.merge(tft_pred, on=['영업장명_메뉴명','h'], how='outer')                 .merge(xgb_pred, on=['영업장명_메뉴명','h'], how='outer').fillna(0.0)\n",
    "    return pred\n",
    "\n",
    "# --- ensemble prediction using weights ---\n",
    "def predict_ensemble_for_test_file(test_file_path: str, weights: dict) -> pd.DataFrame:\n",
    "    df_raw=pd.read_csv(test_file_path, parse_dates=['영업일자'])\n",
    "    df_28=prepare_test(df_raw)\n",
    "    pred=predict_all_models(df_28)\n",
    "    pred['pred_ens'] = (weights['w_cat']*pred.get('pred_cat',0) + weights['w_tft']*pred.get('pred_tft',0) + weights['w_xgb']*pred.get('pred_xgb',0))\n",
    "    pred['file']=os.path.basename(test_file_path)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Ensemble Weight Optimization (Optuna) -- Added\n",
    "# ==============================================================\n",
    "\n",
    "def build_validation_context(train_df):\n",
    "    contexts=[]\n",
    "    targets=[]\n",
    "    for pair, sub in train_df.groupby('영업장명_메뉴명'):\n",
    "        sub=sub.sort_values('영업일자')\n",
    "        if len(sub) < ENC_LEN + PRED_LEN:\n",
    "            continue\n",
    "        hist=sub.iloc[-(ENC_LEN+PRED_LEN):-PRED_LEN].copy()\n",
    "        fut=sub.iloc[-PRED_LEN:]['매출수량'].tolist()\n",
    "        hist['영업장명_메뉴명']=pair\n",
    "        contexts.append(hist)\n",
    "        for h,val in enumerate(fut,1):\n",
    "            targets.append({'영업장명_메뉴명':pair,'h':h,'y_true':float(val)})\n",
    "    ctx=pd.concat(contexts).reset_index(drop=True)\n",
    "    tgt=pd.DataFrame(targets)\n",
    "    return ctx,tgt\n",
    "\n",
    "val_ctx,val_tgt=build_validation_context(train)\n",
    "val_pred=predict_all_models(val_ctx).merge(val_tgt, on=['영업장명_메뉴명','h'])\n",
    "val_pred=val_pred.fillna(0.0)\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(2*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+1e-8))\n",
    "\n",
    "\n",
    "def weight_objective(trial):\n",
    "    w_cat=trial.suggest_float('w_cat',0,1)\n",
    "    w_tft=trial.suggest_float('w_tft',0,1-w_cat)\n",
    "    w_xgb=1 - w_cat - w_tft\n",
    "    if w_xgb < 0:\n",
    "        raise optuna.TrialPruned()\n",
    "    y_hat=w_cat*val_pred['pred_cat'] + w_tft*val_pred['pred_tft'] + w_xgb*val_pred['pred_xgb']\n",
    "    return smape(val_pred['y_true'], y_hat)\n",
    "\n",
    "study_w=optuna.create_study(direction='minimize')\n",
    "study_w.optimize(weight_objective, n_trials=50)\n",
    "best_weights=study_w.best_params\n",
    "best_weights['w_xgb']=1 - best_weights['w_cat'] - best_weights['w_tft']\n",
    "print('Best weights:', best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Prediction 실행\n",
    "# -------------------------------\n",
    "all_preds = []\n",
    "for test_path in sorted(test_paths):\n",
    "    print(f'[Predict] {test_path}')\n",
    "    all_preds.append(predict_ensemble_for_test_file(test_path, best_weights))\n",
    "all_preds = pd.concat(all_preds, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888817bf",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8249e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Submission 변환 함수 (vectorized)\n",
    "# -------------------------------\n",
    "def convert_preds_to_submission(all_preds, sample_submission):\n",
    "    full_pred_df = pd.concat(all_preds, ignore_index=True)\n",
    "    full_pred_df['test_id'] = full_pred_df['file'].str.replace('.csv', '', regex=False)\n",
    "    wide = full_pred_df.pivot_table(index=['test_id','h'], columns='영업장명_메뉴명', values='pred_ens').reset_index()\n",
    "    wide['영업일자'] = wide['test_id'] + '+' + wide['h'].astype(int).astype(str) + '일'\n",
    "    wide = wide.drop(columns=['test_id','h'])\n",
    "    submission = sample_submission.merge(wide, on='영업일자', how='left').fillna(0)\n",
    "    submission = submission[sample_submission.columns]\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Submission 실행\n",
    "# -------------------------------\n",
    "sample_submission = pd.read_csv(sample_path)\n",
    "submission_df = convert_preds_to_submission(all_preds, sample_submission)\n",
    "output_path = 'submission_ensemble_optimized.csv'\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "print('Saved:', output_path)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
