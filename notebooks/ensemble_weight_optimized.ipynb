{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2e3948",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost pytorch-lightning pytorch-forecasting optuna -q"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Drive Mount & Path Setup\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = '/content/drive/MyDrive/LGAI'\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'model')\n",
    "SUB_DIR = os.path.join(BASE_DIR, 'submission')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(SUB_DIR, exist_ok=True)\n",
    "\n",
    "train_path  = os.path.join(BASE_DIR, 'train', 'train.csv')\n",
    "sample_path = os.path.join(BASE_DIR, 'sample_submission.csv')\n",
    "test_paths  = [os.path.join(BASE_DIR, 'test', f'TEST_0{i}.csv') for i in range(10)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, gc, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import RMSE\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "\n",
    "import optuna\n",
    "import glob, re\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc6a89",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed & Setting Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b04bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \uacf5\ud1b5 \uc124\uc815\n",
    "ENC_LEN  = 28\n",
    "PRED_LEN = 7\n",
    "ROLL_WINS = [7, 14, 28]\n",
    "\n",
    "# CatBoost \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\n",
    "CAT_PARAMS = dict(\n",
    "    depth=8,\n",
    "    learning_rate=0.05,\n",
    "    iterations=2000,\n",
    "    loss_function=\"RMSE\",\n",
    "    l2_leaf_reg=5,\n",
    "    random_seed=42,\n",
    "    verbose=200,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    ")\n",
    "\n",
    "# TFT-mini \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 (dropout tuned later)\n",
    "TFT_PARAMS = dict(\n",
    "    learning_rate=2e-3,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=2,\n",
    "    hidden_continuous_size=32,\n",
    "    lstm_layers=1,\n",
    ")\n",
    "\n",
    "# \uc559\uc0c1\ube14 \uac00\uc911\uce58\n",
    "W_CAT = 0.4\n",
    "W_TFT = 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3aef3",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753047b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv(train_path, parse_dates=['\uc601\uc5c5\uc77c\uc790'])\n",
    "sample = pd.read_csv(sample_path)\n",
    "tests  = {f'TEST_0{i}': pd.read_csv(p, parse_dates=['\uc601\uc5c5\uc77c\uc790']) for i, p in enumerate(test_paths)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3afdf",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff71d0f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# (1) \ubbf8\ub798\uc5d0 '\uc54c\uace0 \uc788\ub294' \ub2ec\ub825/\uc774\ubca4\ud2b8 \ud53c\ucc98 \ubaa9\ub85d\n",
    "# --------------------------------------------------------------\n",
    "known_future_cols = [\n",
    "    \"dow\",\"month\",\"is_weekend\",\"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# (2) CatBoost \ud559\uc2b5\uc6a9: shift \ubca1\ud130\ud654 + fragmentation \ubc29\uc9c0\n",
    "# --------------------------------------------------------------\n",
    "train_sorted = train.sort_values([\"\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85\",\"\uc601\uc5c5\uc77c\uc790\"]).reset_index(drop=True)\n",
    "g = train_sorted.groupby(\"\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85\", sort=False)\n",
    "train_sorted[\"hist_ok\"] = g.cumcount() >= (ENC_LEN - 1)\n",
    "\n",
    "base_for_shift = [\"\ub9e4\ucd9c\uc218\ub7c9\"] + known_future_cols\n",
    "shift_blocks = []\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    block = g[base_for_shift].shift(-h)\n",
    "    rename_map = {\"\ub9e4\ucd9c\uc218\ub7c9\": f\"y_H{h}\"}\n",
    "    rename_map.update({c: f\"{c}_H{h}\" for c in known_future_cols})\n",
    "    block = block.rename(columns=rename_map)\n",
    "    shift_blocks.append(block)\n",
    "\n",
    "train_shift = pd.concat(shift_blocks, axis=1)\n",
    "train_sorted = pd.concat([train_sorted, train_shift], axis=1).copy()\n",
    "\n",
    "def build_catboost_Xy_by_shift(df: pd.DataFrame, stride: int = 1):\n",
    "    Xy = {}\n",
    "    base_cols = cat_features_cols + [f\"roll_mean_{w}\" for w in ROLL_WINS]\n",
    "    for c in base_cols:\n",
    "        if c in cat_features_cols:\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    for h in range(1, PRED_LEN + 1):\n",
    "        fut_cols_h = [f\"{c}_H{h}\" for c in known_future_cols]\n",
    "        target_col = f\"y_H{h}\"\n",
    "        mask = df[\"hist_ok\"] & df[target_col].notna()\n",
    "        use = df.loc[mask, base_cols + fut_cols_h + [target_col]].copy() if stride==1 else (\n",
    "            df.loc[\n",
    "                df[mask].groupby(\"\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85\", sort=False).apply(lambda x: x.iloc[::stride]).reset_index(level=0, drop=True).index,\n",
    "                base_cols + fut_cols_h + [target_col]\n",
    "            ].copy()\n",
    "        )\n",
    "        for c in fut_cols_h:\n",
    "            use[c] = pd.to_numeric(use[c], errors=\"coerce\").astype(\"float32\")\n",
    "        use[target_col] = pd.to_numeric(use[target_col], errors=\"coerce\").astype(\"float32\")\n",
    "        X = use[base_cols + fut_cols_h].reset_index(drop=True)\n",
    "        y = use[target_col].reset_index(drop=True)\n",
    "        Xy[h] = (X, y)\n",
    "    return Xy\n",
    "\n",
    "Xy_h = build_catboost_Xy_by_shift(train_sorted, stride=1)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# (3) TFT\uc6a9 \ub370\uc774\ud130\uc14b/\ub85c\ub354\n",
    "# --------------------------------------------------------------\n",
    "train_tft = train.copy()\n",
    "train_tft[\"time_idx\"] = (train_tft[\"\uc601\uc5c5\uc77c\uc790\"] - train_tft[\"\uc601\uc5c5\uc77c\uc790\"].min()).dt.days.astype(int)\n",
    "for c in [\"store_id\", \"item_id\", \"pair_id\"]:\n",
    "    train_tft[c] = train_tft[c].astype(str)\n",
    "\n",
    "static_categoricals = [\"store_id\", \"item_id\", \"pair_id\"]\n",
    "\n",
    "time_varying_known_reals = [\n",
    "    \"time_idx\",\n",
    "    \"dow_sin\",\"dow_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"is_weekend\",\"is_spring\",\"is_summer\",\"is_fall\",\"is_winter\",\n",
    "    \"is_peak_summer\",\"is_peak_winter\",\n",
    "    \"is_holiday\",\"before_holiday\",\"after_holiday\",\"is_holiday_run\",\n",
    "    \"is_summer_vac\",\"is_winter_vac\",\n",
    "    \"EVENT_SF_SZN\",\"EVENT_SUMMER_SZN\",\"EVENT_WINTER_SZN\",\n",
    "    \"is_event_global\",\"near_event_global\",\"is_event_target\",\"near_event_target\",\n",
    "]\n",
    "\n",
    "time_varying_unknown_reals = [\"\ub9e4\ucd9c\uc218\ub7c9\"] + [f\"roll_mean_{w}\" for w in ROLL_WINS]\n",
    "\n",
    "for c in time_varying_known_reals:\n",
    "    if c != \"time_idx\":\n",
    "        train_tft[c] = pd.to_numeric(train_tft[c], errors=\"coerce\").astype(\"float32\")\n",
    "train_tft[\"time_idx\"] = pd.to_numeric(train_tft[\"time_idx\"], errors=\"coerce\").astype(\"int64\")\n",
    "for c in [f\"roll_mean_{w}\" for w in ROLL_WINS] + [\"\ub9e4\ucd9c\uc218\ub7c9\"]:\n",
    "    train_tft[c] = pd.to_numeric(train_tft[c], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "training_cutoff = train_tft[\"time_idx\"].max() - PRED_LEN\n",
    "\n",
    "# --- Modified: added GroupNormalizer ---\n",
    "tft_dataset = TimeSeriesDataSet(\n",
    "    train_tft[train_tft.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"\ub9e4\ucd9c\uc218\ub7c9\",\n",
    "    group_ids=[\"pair_id\"],\n",
    "    min_encoder_length=ENC_LEN,\n",
    "    max_encoder_length=ENC_LEN,\n",
    "    min_prediction_length=PRED_LEN,\n",
    "    max_prediction_length=PRED_LEN,\n",
    "    static_categoricals=static_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"pair_id\"]),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(tft_dataset, train_tft, predict=True, stop_randomization=True)\n",
    "train_loader = tft_dataset.to_dataloader(train=True, batch_size=256, num_workers=2)\n",
    "val_loader   = validation.to_dataloader(train=False, batch_size=256, num_workers=2)\n",
    "\n",
    "print('[Define Dataset] dataset ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# XGBoost \ud559\uc2b5 (h=1..7) -- Added\n",
    "# ==============================================================\n",
    "import xgboost as xgb\n",
    "xgb_models = {}\n",
    "\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    Xh, yh = Xy_h[h]\n",
    "    if len(Xh) == 0:\n",
    "        print(f\"[XGBoost][H{h}] \ud559\uc2b5 \ub370\uc774\ud130\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \uac74\ub108\ub701\ub2c8\ub2e4.\")\n",
    "        continue\n",
    "    X_tr, y_tr, X_va, y_va = split_train_valid(Xh, yh, valid_ratio=0.1)\n",
    "    print(f\"[XGBoost][H{h}] train={len(X_tr):,}  valid={len(X_va):,}\")\n",
    "    model = xgb.XGBRegressor(\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=2000,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=5,\n",
    "        objective='reg:squarederror',\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        enable_categorical=True,\n",
    "    )\n",
    "    model.fit(X_tr, y_tr,\n",
    "              eval_set=[(X_va, y_va)],\n",
    "              verbose=200,\n",
    "              early_stopping_rounds=100)\n",
    "    xgb_models[h] = model\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# CatBoost \ud559\uc2b5 (h=1..7)\n",
    "# ==============================================================\n",
    "if \"split_train_valid\" not in globals():\n",
    "    def split_train_valid(X: pd.DataFrame, y: pd.Series, valid_ratio=0.1):\n",
    "        n = len(X); k = int(n * (1 - valid_ratio))\n",
    "        return (X.iloc[:k].reset_index(drop=True), y.iloc[:k].reset_index(drop=True),\n",
    "                X.iloc[k:].reset_index(drop=True), y.iloc[k:].reset_index(drop=True))\n",
    "\n",
    "if \"cat_col_indices\" not in globals():\n",
    "    def cat_col_indices(cols):\n",
    "        return [i for i, c in enumerate(cols) if c in cat_features_cols]\n",
    "\n",
    "cat_models = {}\n",
    "\n",
    "for h in range(1, PRED_LEN + 1):\n",
    "    Xh, yh = Xy_h[h]\n",
    "    if len(Xh) == 0:\n",
    "        print(f\"[CatBoost][H{h}] \ud559\uc2b5 \ub370\uc774\ud130\uac00 \uc5c6\uc2b5\ub2c8\ub2e4. \uac74\ub108\ub701\ub2c8\ub2e4.\")\n",
    "        continue\n",
    "\n",
    "    # \ubc94\uc8fc\ud615 \ud53c\ucc98 \uc778\ub371\uc2a4\n",
    "    cat_idx = cat_col_indices(Xh.columns)\n",
    "\n",
    "    # \ub2e8\uc21c \uc2dc\uacc4\uc5f4 \ubd84\ud560(\ub4a4\ucabd 10% \uac80\uc99d)\n",
    "    X_tr, y_tr, X_va, y_va = split_train_valid(Xh, yh, valid_ratio=0.1)\n",
    "    print(f\"[CatBoost][H{h}] train={len(X_tr):,}  valid={len(X_va):,}\")\n",
    "\n",
    "    train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
    "    valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n",
    "\n",
    "    # \ubaa8\ub378 \uc0dd\uc131 \ubc0f \ud559\uc2b5\n",
    "    cb = CatBoostRegressor(**CAT_PARAMS)\n",
    "    cb.fit(\n",
    "        train_pool,\n",
    "        eval_set=valid_pool,\n",
    "        use_best_model=True,\n",
    "        verbose=200\n",
    "    )\n",
    "    cat_models[h] = cb\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7560266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# TFT-mini \ud559\uc2b5 (dropout \ucd5c\uc801\ud654 \ud3ec\ud568)\n",
    "# ==============================================================\n",
    "\n",
    "# --- Modified: dropout optimization with Optuna ---\n",
    "def tft_objective(trial):\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    model = TemporalFusionTransformer.from_dataset(\n",
    "        tft_dataset,\n",
    "        loss=RMSE(),\n",
    "        log_interval=200,\n",
    "        reduce_on_plateau_patience=4,\n",
    "        dropout=dropout,\n",
    "        weight_decay=1e-2,\n",
    "        **TFT_PARAMS\n",
    "    )\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=30,\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        precision='bf16-mixed' if torch.cuda.is_bf16_supported() else 32,\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        logger=False,\n",
    "        enable_progress_bar=False,\n",
    "        limit_val_batches=1.0,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    return trainer.callback_metrics['val_loss'].item()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(tft_objective, n_trials=10)\n",
    "best_dropout = study.best_params['dropout']\n",
    "print('Best dropout:', best_dropout)\n",
    "\n",
    "# --- Train final model with optimal dropout ---\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, mode='min')\n",
    "lr_logger  = LearningRateMonitor(logging_interval='epoch')\n",
    "checkpoint = ModelCheckpoint(monitor='val_loss', save_top_k=1, mode='min')\n",
    "logger     = CSVLogger('tft_logs', name='catboost_tft')\n",
    "\n",
    "precision = 'bf16-mixed' if torch.cuda.is_bf16_supported() else 32\n",
    "print(f'[Precision] Using {precision}')\n",
    "\n",
    "tft_model = TemporalFusionTransformer.from_dataset(\n",
    "    tft_dataset,\n",
    "    loss=RMSE(),\n",
    "    log_interval=200,\n",
    "    reduce_on_plateau_patience=4,\n",
    "    dropout=best_dropout,\n",
    "    weight_decay=1e-2,\n",
    "    **TFT_PARAMS\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    precision=precision,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop, lr_logger, checkpoint],\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True,\n",
    "    limit_val_batches=1.0,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(tft_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "if checkpoint.best_model_path:\n",
    "    tft_model = TemporalFusionTransformer.load_from_checkpoint(checkpoint.best_model_path)\n",
    "    print('[TFT] best checkpoint:', checkpoint.best_model_path)\n",
    "else:\n",
    "    print('[TFT] Warning: no best checkpoint found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1066b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d23451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Prediction Functions (CatBoost, TFT-mini, XGBoost) -- Modified\n",
    "# ==============================================================\n",
    "\n",
    "def convert_to_submission_format(pred_df: pd.DataFrame, sample_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = sample_df.copy()\n",
    "    wide = pred_df.pivot(index='\uc601\uc5c5\uc77c\uc790', columns='\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85', values='\ub9e4\ucd9c\uc218\ub7c9')\n",
    "    for r in range(len(out)):\n",
    "        date_key = out.at[r, '\uc601\uc5c5\uc77c\uc790']\n",
    "        if date_key in wide.index:\n",
    "            for c in out.columns:\n",
    "                if c == '\uc601\uc5c5\uc77c\uc790':\n",
    "                    continue\n",
    "                if c in wide.columns:\n",
    "                    val = wide.at[date_key, c]\n",
    "                    out.at[r, c] = 0 if pd.isna(val) else max(float(val), 0.0)\n",
    "    for c in out.columns:\n",
    "        if c == '\uc601\uc5c5\uc77c\uc790':\n",
    "            continue\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce').fillna(0).clip(lower=0)\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_test(df_test_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_test_raw.copy()\n",
    "    df['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'] = df['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'].astype(str)\n",
    "    df['\uc5c5\uc7a5\uba85'] = df['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'].str.split('_', n=1).str[0]\n",
    "    df['\uba54\ub274\uba85'] = df['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'].str.split('_', n=1).str[1].fillna('NA')\n",
    "    df['store_id'] = df['\uc5c5\uc7a5\uba85'].map(store2id).fillna(-1).astype(int).astype(str)\n",
    "    df['item_id']  = df['\uba54\ub274\uba85'].map(item2id).fillna(-1).astype(int).astype(str)\n",
    "    df['pair_id']  = (df['\uc5c5\uc7a5\uba85'] + '###' + df['\uba54\ub274\uba85']).map(pair2id).fillna(-1).astype(int).astype(str)\n",
    "    df = df.sort_values(['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85','\uc601\uc5c5\uc77c\uc790']).reset_index(drop=True)\n",
    "    df = add_domain_features(df)\n",
    "    for w in ROLL_WINS:\n",
    "        df[f'roll_mean_{w}'] = (\n",
    "            df.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85')['\ub9e4\ucd9c\uc218\ub7c9']\n",
    "              .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# --- CatBoost prediction ---\n",
    "def predict_catboost(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds=[]\n",
    "    for pair, sub in df_28.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'):\n",
    "        sub=sub.sort_values('\uc601\uc5c5\uc77c\uc790').reset_index(drop=True)\n",
    "        history=sub.iloc[-ENC_LEN:].copy()\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            fut_date = history['\uc601\uc5c5\uc77c\uc790'].iloc[-1] + timedelta(days=1)\n",
    "            fut_row=pd.DataFrame([{ '\uc601\uc5c5\uc77c\uc790':fut_date,\n",
    "                                    '\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair,\n",
    "                                    '\uc5c5\uc7a5\uba85':history['\uc5c5\uc7a5\uba85'].iloc[-1],\n",
    "                                    '\uba54\ub274\uba85':history['\uba54\ub274\uba85'].iloc[-1],\n",
    "                                    'store_id':history['store_id'].iloc[-1],\n",
    "                                    'item_id':history['item_id'].iloc[-1],\n",
    "                                    'pair_id':history['pair_id'].iloc[-1],\n",
    "                                    '\ub9e4\ucd9c\uc218\ub7c9':0 }])\n",
    "            fut_row=add_domain_features(fut_row)\n",
    "            tmp_hist=pd.concat([history, fut_row], ignore_index=True)\n",
    "            for w in ROLL_WINS:\n",
    "                tmp_hist[f'roll_mean_{w}'] = (\n",
    "                    tmp_hist.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85')['\ub9e4\ucd9c\uc218\ub7c9']\n",
    "                           .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            fut_row=tmp_hist.iloc[[-1]]\n",
    "            X=fut_row[cat_features_cols + [f'roll_mean_{w}' for w in ROLL_WINS]].copy()\n",
    "            fut_feats=fut_row[known_future_cols].copy(); fut_feats.columns=[c+f'_H{h}' for c in fut_feats.columns]\n",
    "            X=pd.concat([X.reset_index(drop=True), fut_feats.reset_index(drop=True)], axis=1)\n",
    "            model=cat_models[h]; cat_idx=[i for i,c in enumerate(X.columns) if c in cat_features_cols]\n",
    "            yhat=float(model.predict(Pool(X, cat_features=cat_idx))[0])\n",
    "            yhat=max(0.0, yhat)\n",
    "            preds.append({'\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair,'h':h,'pred_cat':yhat})\n",
    "            fut_row.loc[:, '\ub9e4\ucd9c\uc218\ub7c9']=yhat\n",
    "            history=pd.concat([history, fut_row], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "# --- XGBoost prediction ---\n",
    "def predict_xgboost(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    preds=[]\n",
    "    for pair, sub in df_28.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'):\n",
    "        sub=sub.sort_values('\uc601\uc5c5\uc77c\uc790').reset_index(drop=True)\n",
    "        history=sub.iloc[-ENC_LEN:].copy()\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            fut_date=history['\uc601\uc5c5\uc77c\uc790'].iloc[-1] + timedelta(days=1)\n",
    "            fut_row=pd.DataFrame([{ '\uc601\uc5c5\uc77c\uc790':fut_date,\n",
    "                                    '\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair,\n",
    "                                    '\uc5c5\uc7a5\uba85':history['\uc5c5\uc7a5\uba85'].iloc[-1],\n",
    "                                    '\uba54\ub274\uba85':history['\uba54\ub274\uba85'].iloc[-1],\n",
    "                                    'store_id':history['store_id'].iloc[-1],\n",
    "                                    'item_id':history['item_id'].iloc[-1],\n",
    "                                    'pair_id':history['pair_id'].iloc[-1],\n",
    "                                    '\ub9e4\ucd9c\uc218\ub7c9':0 }])\n",
    "            fut_row=add_domain_features(fut_row)\n",
    "            tmp_hist=pd.concat([history, fut_row], ignore_index=True)\n",
    "            for w in ROLL_WINS:\n",
    "                tmp_hist[f'roll_mean_{w}'] = (\n",
    "                    tmp_hist.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85')['\ub9e4\ucd9c\uc218\ub7c9']\n",
    "                           .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "                )\n",
    "            fut_row=tmp_hist.iloc[[-1]]\n",
    "            X=fut_row[cat_features_cols + [f'roll_mean_{w}' for w in ROLL_WINS]].copy()\n",
    "            fut_feats=fut_row[known_future_cols].copy(); fut_feats.columns=[c+f'_H{h}' for c in fut_feats.columns]\n",
    "            X=pd.concat([X.reset_index(drop=True), fut_feats.reset_index(drop=True)], axis=1)\n",
    "            model=xgb_models[h]\n",
    "            yhat=float(model.predict(X)[0])\n",
    "            yhat=max(0.0, yhat)\n",
    "            preds.append({'\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair,'h':h,'pred_xgb':yhat})\n",
    "            fut_row.loc[:, '\ub9e4\ucd9c\uc218\ub7c9']=yhat\n",
    "            history=pd.concat([history, fut_row], ignore_index=True)\n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "# --- TFT prediction ---\n",
    "def predict_tft(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp=df_28.copy()\n",
    "    tmp['time_idx']=(tmp['\uc601\uc5c5\uc77c\uc790'] - train['\uc601\uc5c5\uc77c\uc790'].min()).dt.days.astype(int)\n",
    "    rows=[]\n",
    "    for pair, sub in tmp.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'):\n",
    "        sub=sub.sort_values('\uc601\uc5c5\uc77c\uc790')\n",
    "        last_date=sub['\uc601\uc5c5\uc77c\uc790'].iloc[-1]\n",
    "        pid=sub['pair_id'].iloc[-1]; sid=sub['store_id'].iloc[-1]; iid=sub['item_id'].iloc[-1]\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            d=last_date + timedelta(days=h)\n",
    "            rows.append({'\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair,'\uc601\uc5c5\uc77c\uc790':d,'pair_id':pid,'store_id':sid,'item_id':iid})\n",
    "    fut=pd.DataFrame(rows)\n",
    "    fut=add_domain_features(fut.assign(\ub9e4\ucd9c\uc218\ub7c9=0))\n",
    "    for w in ROLL_WINS:\n",
    "        fut[f'roll_mean_{w}']=(\n",
    "            fut.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85')['\ub9e4\ucd9c\uc218\ub7c9']\n",
    "               .transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
    "               .fillna(0)\n",
    "        )\n",
    "    fut['time_idx']=(fut['\uc601\uc5c5\uc77c\uc790'] - train['\uc601\uc5c5\uc77c\uc790'].min()).dt.days.astype(int)\n",
    "    enc_dec=pd.concat([tmp, fut], ignore_index=True)\n",
    "    enc_dec=enc_dec.fillna(0)\n",
    "    predict_ds=TimeSeriesDataSet.from_dataset(tft_dataset, enc_dec, predict=True, stop_randomization=True)\n",
    "    predict_loader=predict_ds.to_dataloader(train=False, batch_size=256, num_workers=2)\n",
    "    yhat=tft_model.predict(predict_loader, mode='prediction')\n",
    "    if isinstance(yhat, tuple): yhat=yhat[0]\n",
    "    fut_sorted=fut.sort_values(['pair_id','\uc601\uc5c5\uc77c\uc790']).reset_index(drop=True)\n",
    "    series_ids=fut_sorted['pair_id'].unique().tolist()\n",
    "    out=[]\n",
    "    for i,pid in enumerate(series_ids):\n",
    "        pair_name=df_28.loc[df_28['pair_id']==pid, '\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'].iloc[0]\n",
    "        for h in range(1, PRED_LEN+1):\n",
    "            out.append({'\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair_name,'h':h,'pred_tft':float(max(0.0, yhat[i, h-1].item()))})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# --- helper to get predictions from all models ---\n",
    "def predict_all_models(df_28: pd.DataFrame) -> pd.DataFrame:\n",
    "    cat_pred=predict_catboost(df_28)\n",
    "    tft_pred=predict_tft(df_28)\n",
    "    xgb_pred=predict_xgboost(df_28)\n",
    "    pred=cat_pred.merge(tft_pred, on=['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85','h'], how='outer')                 .merge(xgb_pred, on=['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85','h'], how='outer').fillna(0.0)\n",
    "    return pred\n",
    "\n",
    "# --- ensemble prediction using weights ---\n",
    "def predict_ensemble_for_test_file(test_file_path: str, weights: dict) -> pd.DataFrame:\n",
    "    df_raw=pd.read_csv(test_file_path, parse_dates=['\uc601\uc5c5\uc77c\uc790'])\n",
    "    df_28=prepare_test(df_raw)\n",
    "    pred=predict_all_models(df_28)\n",
    "    pred['pred_ens'] = (weights['w_cat']*pred.get('pred_cat',0) + weights['w_tft']*pred.get('pred_tft',0) + weights['w_xgb']*pred.get('pred_xgb',0))\n",
    "    pred['file']=os.path.basename(test_file_path)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Ensemble Weight Optimization (Optuna) -- Added\n",
    "# ==============================================================\n",
    "\n",
    "def build_validation_context(train_df):\n",
    "    contexts=[]\n",
    "    targets=[]\n",
    "    for pair, sub in train_df.groupby('\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85'):\n",
    "        sub=sub.sort_values('\uc601\uc5c5\uc77c\uc790')\n",
    "        if len(sub) < ENC_LEN + PRED_LEN:\n",
    "            continue\n",
    "        hist=sub.iloc[-(ENC_LEN+PRED_LEN):-PRED_LEN].copy()\n",
    "        fut=sub.iloc[-PRED_LEN:]['\ub9e4\ucd9c\uc218\ub7c9'].tolist()\n",
    "        hist['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85']=pair\n",
    "        contexts.append(hist)\n",
    "        for h,val in enumerate(fut,1):\n",
    "            targets.append({'\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85':pair,'h':h,'y_true':float(val)})\n",
    "    ctx=pd.concat(contexts).reset_index(drop=True)\n",
    "    tgt=pd.DataFrame(targets)\n",
    "    return ctx,tgt\n",
    "\n",
    "val_ctx,val_tgt=build_validation_context(train)\n",
    "val_pred=predict_all_models(val_ctx).merge(val_tgt, on=['\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85','h'])\n",
    "val_pred=val_pred.fillna(0.0)\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(2*np.abs(y_true - y_pred)/(np.abs(y_true)+np.abs(y_pred)+1e-8))\n",
    "\n",
    "\n",
    "def weight_objective(trial):\n",
    "    w_cat=trial.suggest_float('w_cat',0,1)\n",
    "    w_tft=trial.suggest_float('w_tft',0,1-w_cat)\n",
    "    w_xgb=1 - w_cat - w_tft\n",
    "    if w_xgb < 0:\n",
    "        raise optuna.TrialPruned()\n",
    "    y_hat=w_cat*val_pred['pred_cat'] + w_tft*val_pred['pred_tft'] + w_xgb*val_pred['pred_xgb']\n",
    "    return smape(val_pred['y_true'], y_hat)\n",
    "\n",
    "study_w=optuna.create_study(direction='minimize')\n",
    "study_w.optimize(weight_objective, n_trials=50)\n",
    "best_weights=study_w.best_params\n",
    "best_weights['w_xgb']=1 - best_weights['w_cat'] - best_weights['w_tft']\n",
    "print('Best weights:', best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Prediction \uc2e4\ud589\n",
    "# -------------------------------\n",
    "all_preds = []\n",
    "for test_path in sorted(test_paths):\n",
    "    print(f'[Predict] {test_path}')\n",
    "    all_preds.append(predict_ensemble_for_test_file(test_path, best_weights))\n",
    "all_preds = pd.concat(all_preds, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888817bf",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8249e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Submission \ubcc0\ud658 \ud568\uc218 (vectorized)\n",
    "# -------------------------------\n",
    "def convert_preds_to_submission(all_preds, sample_submission):\n",
    "    full_pred_df = pd.concat(all_preds, ignore_index=True)\n",
    "    full_pred_df['test_id'] = full_pred_df['file'].str.replace('.csv', '', regex=False)\n",
    "    wide = full_pred_df.pivot_table(index=['test_id','h'], columns='\uc601\uc5c5\uc7a5\uba85_\uba54\ub274\uba85', values='pred_ens').reset_index()\n",
    "    wide['\uc601\uc5c5\uc77c\uc790'] = wide['test_id'] + '+' + wide['h'].astype(int).astype(str) + '\uc77c'\n",
    "    wide = wide.drop(columns=['test_id','h'])\n",
    "    submission = sample_submission.merge(wide, on='\uc601\uc5c5\uc77c\uc790', how='left').fillna(0)\n",
    "    submission = submission[sample_submission.columns]\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "sample_submission = pd.read_csv(sample_path)\n",
    "submission_df = convert_preds_to_submission(all_preds, sample_submission)\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "output_path = os.path.join(SUB_DIR, f'submission_{ts}.csv')\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "print('Saved:', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained models to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save models and ensemble weights with timestamp\n",
    "from datetime import datetime\n",
    "import shutil, json\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "cat_path = os.path.join(MODEL_DIR, f'catboost_model_{ts}.cbm')\n",
    "xgb_path = os.path.join(MODEL_DIR, f'xgb_model_{ts}.json')\n",
    "tft_path = os.path.join(MODEL_DIR, f'tft_model_{ts}.ckpt')\n",
    "weights_path = os.path.join(MODEL_DIR, f'ensemble_weights_{ts}.json')\n",
    "joblib.dump(cat_models, cat_path)\n",
    "joblib.dump(xgb_models, xgb_path)\n",
    "if checkpoint.best_model_path:\n",
    "    shutil.copy(checkpoint.best_model_path, tft_path)\n",
    "with open(weights_path, 'w') as f:\n",
    "    json.dump(best_weights, f)\n",
    "print('Saved models to', MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved models and run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load latest saved models and run inference\n",
    "import glob, json\n",
    "def load_latest(prefix):\n",
    "    files = sorted(glob.glob(os.path.join(MODEL_DIR, f\"{prefix}*\")))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "cat_file = load_latest('catboost_model_')\n",
    "xgb_file = load_latest('xgb_model_')\n",
    "tft_file = load_latest('tft_model_')\n",
    "weights_file = load_latest('ensemble_weights_')\n",
    "cat_models = joblib.load(cat_file)\n",
    "xgb_models = joblib.load(xgb_file)\n",
    "tft_model = TemporalFusionTransformer.load_from_checkpoint(tft_file)\n",
    "with open(weights_file) as f:\n",
    "    best_weights = json.load(f)\n",
    "all_preds = []\n",
    "for test_path in sorted(test_paths):\n",
    "    all_preds.append(predict_ensemble_for_test_file(test_path, best_weights))\n",
    "all_preds = pd.concat(all_preds, ignore_index=True)\n",
    "sample_submission = pd.read_csv(sample_path)\n",
    "submission_df = convert_preds_to_submission(all_preds, sample_submission)\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "output_path = os.path.join(SUB_DIR, f'submission_{ts}.csv')\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "print('Saved:', output_path)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}